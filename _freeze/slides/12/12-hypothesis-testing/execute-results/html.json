{
  "hash": "c37c5833445ecc93d48e37fffaacf22a",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: Hypothesis Testing with Randomization, part two\nsubtitle: Chapter 11\nformat: revealjs\nauto-stretch: false\nauthor: \"\"\necho: false\n---\n\n::: {.cell}\n\n:::\n\n\n\n\n## In groups\n\n-   Discuss Homework: Chapter 11, #3, 5, 7\n\n## Gender Discrimination Study\n\nOn Tuesday we looked at a study on gender discrimination. The study observed that $87.5\\%$ of male candidates were promoted whereas $58.3\\%$ of female candidates were promoted.\n\n\\\n\n\n\n\n::: {.cell}\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 4 × 4\n# Groups:   gender [2]\n  gender decision         n  prop\n  <fct>  <fct>        <int> <dbl>\n1 male   promoted        21 0.875\n2 male   not promoted     3 0.125\n3 female promoted        14 0.583\n4 female not promoted    10 0.417\n```\n\n\n:::\n:::\n\n\n\n\n\\\nDifference in proportions: $0.583 - 0.875 = -0.292$\n\n## Test Statistic\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndiff_orig <- gender_discrimination |>\n  group_by(gender) |>\n  summarize(prop_promoted = mean(decision == \"promoted\")) |>\n  summarize(stat = diff(prop_promoted))\n\n# see the result\ndiff_orig\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 1 × 1\n    stat\n   <dbl>\n1 -0.292\n```\n\n\n:::\n:::\n\n\n\n\n## State the Hypotheses\n\n-   $H_0$ there is no difference between the two groups. Men and women are promoted at the same rate.\n\n-   $H_A$ there **is** a difference. Men are promoted more often than women.\n\n\\\n\nThe study observed a difference of $-0.292$. How unusual would this be **if the null hypothesis is true**?\n\n\\\nIf it's very unusual then we have evidence to **reject** $H_0$\n\n## Simulation\n\n-   Randomly shuffle a deck of 48 cards that has 35 \"promoted\" and 13 \"not promoted\"\n-   Deal into two groups\n-   Calculate new difference in proportions\n-   Repeat!\n-   Look at the distribution of simulation results\n\n## Simulation with R using `infer` package\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nset.seed(1234)\n\ngender_discrimination_perm <- gender_discrimination |>\n  specify(decision ~ gender, success = \"promoted\") |>\n  hypothesize(null = \"independence\") |>\n  generate(reps = 1000, type = \"permute\") |>\n  calculate(stat = \"diff in props\", order = c(\"male\", \"female\"))\n```\n:::\n\n\n\n\n-   `specify` -- what variables are we looking at?\n\n-   `hypothesize` -- what kind of hypothesis test are we doing?\n\n-   `generate` -- do a simulation (many times)\n\n-   `calculate` -- results of the simulation\n\n## Visualize results\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ngender_discrimination_perm |>\n  visualize(bins=30) +\n  shade_p_value(obs_stat = diff_orig, direction = \"less\")\n```\n\n::: {.cell-output-display}\n![](12-hypothesis-testing_files/figure-revealjs/unnamed-chunk-5-1.png){width=960}\n:::\n:::\n\n\n\n\n## p-value\n\n*probability of observing data as (or more) extreme than what we actually got, assuming the null hypothesis is true*\n\n\\\n\nIn our example, we would want to know how many of the simulated differences were **less than or equal** to $-0.29$\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ngender_discrimination_perm |>\n  filter( stat <= -0.29 ) |>\n  nrow()\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 26\n```\n\n\n:::\n:::\n\n\n\n\n\\\nIn other words, our observed value of $-0.29$ happened in the simulations 26 times, or w/ probability $26/1000 = 0.026$\n\n## Compute p-value with R\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ngender_discrimination_perm |>\n  get_p_value(obs_stat = diff_orig, direction = \"less\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 1 × 1\n  p_value\n    <dbl>\n1   0.026\n```\n\n\n:::\n:::\n\n::: {.cell}\n::: {.cell-output-display}\n![](12-hypothesis-testing_files/figure-revealjs/unnamed-chunk-8-1.png){width=960}\n:::\n:::\n\n\n\n\n## Discernability Level\n\n-   How much evidence do we require to reject $H_0$?\n\n-   How skeptical are we?\n\n-   What threshold should we compare **p-value** to?\n\nCommonly used thresholds (**discernability level**)\\\n$$\n0.1, \\,  0.05, \\,  0.01\n$$\n \n\\\nNote:  these are also known as *significance* levels\n\n## $\\alpha = 0.05$\n\nFor a discernability level of $0.05$, we would conclude that since $p < \\alpha$, we have sufficient evidence to reject $H_0$.\n\n\\\n\n> We would say that the data provides **statistically discernable** evidence against the null hypothesis.\n\n\\\nNote: this is often phased as the data being statistically *significant*\n\n## Why $0.05$?\n\nA discernability level of $0.05$ is very common. Why is this?\n\n<https://www.openintro.org/book/stat/why05>\n",
    "supporting": [
      "12-hypothesis-testing_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {
      "include-after-body": [
        "\n<script>\n  // htmlwidgets need to know to resize themselves when slides are shown/hidden.\n  // Fire the \"slideenter\" event (handled by htmlwidgets.js) when the current\n  // slide changes (different for each slide format).\n  (function () {\n    // dispatch for htmlwidgets\n    function fireSlideEnter() {\n      const event = window.document.createEvent(\"Event\");\n      event.initEvent(\"slideenter\", true, true);\n      window.document.dispatchEvent(event);\n    }\n\n    function fireSlideChanged(previousSlide, currentSlide) {\n      fireSlideEnter();\n\n      // dispatch for shiny\n      if (window.jQuery) {\n        if (previousSlide) {\n          window.jQuery(previousSlide).trigger(\"hidden\");\n        }\n        if (currentSlide) {\n          window.jQuery(currentSlide).trigger(\"shown\");\n        }\n      }\n    }\n\n    // hookup for slidy\n    if (window.w3c_slidy) {\n      window.w3c_slidy.add_observer(function (slide_num) {\n        // slide_num starts at position 1\n        fireSlideChanged(null, w3c_slidy.slides[slide_num - 1]);\n      });\n    }\n\n  })();\n</script>\n\n"
      ]
    },
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}